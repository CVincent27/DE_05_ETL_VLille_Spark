{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1t3It5EMj-jjtPd6VVB41VnFELJWKMltc","authorship_tag":"ABX9TyNNze7V93Hj9aZozCcPzaUX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!apt-get update\n","!pip install pyspark findspark"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ah9rYPbFY4i7","executionInfo":{"status":"ok","timestamp":1742806957228,"user_tz":-60,"elapsed":9581,"user":{"displayName":"Vincent Cardon","userId":"10862714883627737653"}},"outputId":"26ec0448-e239-4290-efc5-765e3d1a8b6d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\r0% [Working]\r            \rHit:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n","Get:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n","Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n","Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n","Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n","Get:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n","Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n","Hit:8 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n","Get:9 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,679 kB]\n","Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n","Get:11 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n","Get:12 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,692 kB]\n","Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,003 kB]\n","Get:14 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,767 kB]\n","Get:15 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,237 kB]\n","Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,535 kB]\n","Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [3,962 kB]\n","Get:18 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [82.7 kB]\n","Get:19 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [35.2 kB]\n","Fetched 24.4 MB in 4s (5,731 kB/s)\n","Reading package lists... Done\n","W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n","Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.5)\n","Collecting findspark\n","  Downloading findspark-2.0.1-py2.py3-none-any.whl.metadata (352 bytes)\n","Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n","Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n","Installing collected packages: findspark\n","Successfully installed findspark-2.0.1\n"]}]},{"cell_type":"markdown","source":["# Configuration et init Spark"],"metadata":{"id":"25FzX6qMYT0t"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ypTu1mqhYS9O"},"outputs":[],"source":["import findspark\n","findspark.init()\n","\n","from pyspark.sql import SparkSession\n","import json\n","import os\n","\n","CONFIG_PATH = \"/content/spark_config.json\"\n","\n","def init_spark():\n","    spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n","    sc = spark.sparkContext\n","\n","    # Sauvegarde de la configuration Spark\n","    config = spark.sparkContext.getConf().getAll()\n","    with open(CONFIG_PATH, \"w\") as f:\n","        json.dump(dict(config), f)\n","\n","    print(\"Spark init\")\n","    return spark, sc\n","\n","def load_spark():\n","    if not os.path.exists(CONFIG_PATH):\n","        raise FileNotFoundError(\"Lancer d'abord `init_spark()`.\")\n","\n","    # Charger config\n","    with open(CONFIG_PATH, \"r\") as f:\n","        config = json.load(f)\n","\n","    # Recréer session spark avec la même config\n","    spark_builder = SparkSession.builder.master(\"local[*]\")\n","    for k, v in config.items():\n","        spark_builder = spark_builder.config(k, v)\n","\n","    spark = spark_builder.getOrCreate()\n","    sc = spark.sparkContext\n","    print(\"Spark chargé avec la config sauvegardée\")\n","    return spark, sc\n"]}]}